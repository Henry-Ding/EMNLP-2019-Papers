{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "with open('oral_paper_by_topic.json') as json_file:\n",
    "    oral_papers = json.load(json_file)\n",
    "with open('poster_demo_paper_by_topic.json') as json_file:\n",
    "    poster_demo_papers = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Machine Learning': ['Attending to Future Tokens for Bidirectional Sequence Generation (#1443)',\n",
       "  'Attention is Not Not Explanation (#526)',\n",
       "  'Practical Obstacles to Deploying Active Learning (#1176)',\n",
       "  'Transfer Learning Between Related Tasks Using Expected Label Proportions (#1207)',\n",
       "  'Insertion-based Decoding with automatically Inferred Generation Order (#TACL-1732)',\n",
       "  'Are We Modeling the Task or the Annotator? An Investigation of Annotator Bias in Natural Language Understanding Datasets (#1092)',\n",
       "  'Robust Text Classifier on Test-Time Budgets (#1128)',\n",
       "  'Commonsense Knowledge Mining from Pretrained Models (#3289)',\n",
       "  'RNN Architecture Learning with Sparse Regularization (#3428)',\n",
       "  'Universal Trigger Sequences for Attacking and Analyzing NLP (#1515)',\n",
       "  'To Annotate or Not? Unsupervised Prediction of Performance Drop due to Domain Shift (#2756)',\n",
       "  'Adaptively Sparse Transformers (#2900)',\n",
       "  'Show Your Work: Improved Reporting of Experimental Results (#3277)',\n",
       "  'A Deep Factorization of Style and Structure in Fonts (#3999)'],\n",
       " 'Lexical Semantics': ['Knowledge Enhanced Contextual Word Representations (#3403)',\n",
       "  'How Contextual are Contextualized Word Representations? (#208)',\n",
       "  'Room to Glo: A Systematic Comparison of Semantic Change Detection Approaches with Word Embeddings (#783)',\n",
       "  'On Correlations between Word Vector Sets (#3976)',\n",
       "  'Game Theory Meets Embeddings: a Unified Framework for Word Sense Disambiguation (#1724)',\n",
       "  'Cross-lingual Semantic Specialization via Lexical Relation Induction (#1735)',\n",
       "  'Modelling the interplay of metaphor and emotion through multitask learning (#2670)',\n",
       "  'How well do NLI models capture verb veridicality? (#3460)',\n",
       "  'Modeling Color?Terminology Across Thousands of Languages (#3515)',\n",
       "  'Negative Focus Detection via Contextual Attention Mechanisms (#1314)',\n",
       "  'Exploring Human Gender Stereotypes with Word Association Test (#1912)',\n",
       "  'Still a Pain in the Neck: Evaluating Text Representations on Lexical Composition (#TACL-1729)',\n",
       "  'Where’s My Head? Definition, Dataset and Models for Numeric Fused-Heads Identification and Resolution (#TACL-1648)'],\n",
       " 'Dialog and Interactive Systems': ['Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog (#166)',\n",
       "  'Multi-hop Selector Network for Multi-turn Response Selection in Retrieval-based Chatbots (#554)',\n",
       "  'MoEL: Mixture of Empathetic Listeners (#1053)',\n",
       "  'Entity-Consistent End-to-end Task-Oriented Dialogue System with KB Retriever (#2430)',\n",
       "  'Building Task-Oriented Visual Dialog Systems Through Alternative Optimization Between Dialog Policy and Language Generation (#3756)',\n",
       "  'TaskMaster Dialog Corpus: Toward a Realistic and Diverse Dataset (#510)',\n",
       "  'MultiDoGO: Multi-Domain Goal-Oriented Dialogues (#1564)',\n",
       "  'Build it Break it Fix it for Dialogue Safety: Robustness from Adversarial Human Attack (#1186)',\n",
       "  'GECOR: An End-to-End Generative Ellipsis and Co-reference Resolution Model for Task-Oriented Dialogue (#1853)',\n",
       "  'Task-Oriented Conversation Generation Using Heterogeneous Memory Networks (#496)'],\n",
       " 'Sentiment Analysis and Argument Mining': ['DialogueGCN: A Graph-based Network for Emotion Recognition in Conversation (#2092)',\n",
       "  'Knowledge-Enriched Transformer for Emotion Detection in Textual Conversations (#1814)',\n",
       "  'Interpretable Relevant Emotion Ranking with Event-Driven Attention (#3544)',\n",
       "  'Justifying Recommendations using Distantly-Labeled Reviews and Fined-Grained Aspects (#518)',\n",
       "  'Using Customer Service Dialogues for Satisfaction Analysis with Context-Assisted Multiple Instance Learning (#204)',\n",
       "  'What Gets Echoed? Understanding the “Pointers” in Explanations of Persuasive Arguments (#2089)',\n",
       "  'Modeling Frames in Argumentation (#2267)',\n",
       "  'AMPERSAND: Argument Mining for PERSuAsive oNline Discussions (#3321)',\n",
       "  'Evaluating adversarial attacks against multiple fact verification systems (#427)',\n",
       "  'Nonsense!: Quality Control via Two-Step Reason Selection for Annotating Local Acceptability and Related Attributes in News Editorials (#564)',\n",
       "  'On the Importance of Delexicalization for Fact Verification (#2984)',\n",
       "  'Towards Debiasing Fact Verification Models (#3338)',\n",
       "  'Recognizing Conflict Opinions in Aspect-level Sentiment Classification with Dual Attention Networks (#911)',\n",
       "  'Investigating Dynamic Routing in Tree-Structured LSTM for Sentiment Analysis (#1395)',\n",
       "  'Aspect-based Sentiment Classification with Aspect-specific Graph Convolutional Networks (#381)',\n",
       "  'Coupling Global and Local Context for Unsupervised Aspect Extraction (#1988)',\n",
       "  'Transferable End-to-End Aspect-based Sentiment Analysis with Selective Adversarial Learning (#65)',\n",
       "  'CAN: Constrained Attention Networks for Multi-Aspect Sentiment Analysis (#1995)',\n",
       "  'Leveraging Just a Few Keywords for Fine-Grained Aspect Detection Through Weakly Supervised Co-Training (#3207)'],\n",
       " 'Summarization and Generation': ['Neural Text Summarization: A Critical Evaluation (#3687)',\n",
       "  'Neural data-to-text generation: A comparison between pipeline and end-to-end architectures (#2586)',\n",
       "  'MoverScore: Text Generation Evaluating with Contextualized Embeddings and Earth Mover Distance (#1175)',\n",
       "  'Select and Attend: Towards Controllable Content Selection in Text Generation (#3049)',\n",
       "  'Sentence-Level Content Planning and Style Specification for Neural Text Generation (#3357)'],\n",
       " 'Sentence-level Semantics': ['Translate and Label! An Encoder-Decoder Approach for Cross-lingual Semantic Role Labeling (#2740)',\n",
       "  'Syntax-Enhanced Self-Attention-Based Semantic Role Labeling (#2106)',\n",
       "  'VerbAtlas: a Novel Large-Scale Verbal Semantic Resource and Its Application to Semantic Role Labeling (#2213)',\n",
       "  'Parameter-free Sentence Embedding via Orthogonal Basis (#1099)',\n",
       "  'Evaluation Benchmarks and Learning Criteria for Discourse-Aware Sentence Representations (#3807)',\n",
       "  'Learning Semantic Parsers from Denotations with Latent Structured Alignments and Abstract Programs (#2676)',\n",
       "  'Broad-Coverage Semantic Parsing as Transduction (#263)',\n",
       "  'Core Semantic First: A Top-down Approach for AMR Parsing (#1544)',\n",
       "  'Don’t paraphrase, detect! Rapid and Effective Data Collection for Semantic Parsing (#2904)',\n",
       "  'Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond (#TACL-1742)'],\n",
       " 'Speech, Vision, Robotics, Multimodal and Grounding': ['Extracting Possessions from Social Media: Images Complement Language (#3013)',\n",
       "  'Learning to Speak and Act in a Fantasy Text Adventure Game (#1243)',\n",
       "  'Help, Anna! Vision-based Navigation with Natural Multimodal Assistance via Retrospective Curiosity-Encouraging Imitation Learning (#1542)',\n",
       "  'Incorporating Visual Semantics into Sentence Representations within a Grounded Space (#2247)',\n",
       "  'Neural Naturalist: Generating Fine-Grained Image Comparisons (#3024)',\n",
       "  'LXMERT: Learning Cross-Modality Encoder Representations from Transformers (#3048)',\n",
       "  'Phrase Grounding by Soft-Label Chain Conditional Random Field (#3765)',\n",
       "  'What You See is What You Get: Visual Pronoun Coreference Resolution in Conversations (#549)',\n",
       "  'YouMakeup: A Large-Scale Domain-Specific Multimodal Dataset for Fine-Grained Semantic Comprehension (#122)',\n",
       "  'DEBUG: A Dense Bottom-Up Grounding Approach for Natural Language Video Localization (#167)'],\n",
       " 'Information Extraction': ['Fine-Grained Evaluation for Entity Linking (#116)',\n",
       "  'Supervising Unsupervised Open Information Extraction Models (#3069)',\n",
       "  'Neural Cross-Lingual Event Detection with Minimal Parallel Resources (#1723)',\n",
       "  'KnowledgeNet: A Benchmark Dataset for Knowledge Base Population (#1258)',\n",
       "  'Effective Use of Transformer Networks for Entity Tracking (#3308)',\n",
       "  'Improving Distantly-Supervised Relation Extraction with Joint Label Embedding (#337)',\n",
       "  'Leverage Lexical Knowledge for Chinese Named Entity Recognition via Collaborative Graph Network (#566)',\n",
       "  'Looking Beyond Label Noise: Shifted Label Distribution Matters in Distantly Supervised Relation Extraction (#1057)',\n",
       "  'Easy First Relation Extraction with Information Redundancy (#1640)',\n",
       "  'Dependency-Guided LSTM-CRF for Named Entity Recognition (#2509)',\n",
       "  'CrossWeigh: Training Named Entity Tagger from Imperfect Annotations (#2712)',\n",
       "  'A Little Annotation does a Lot of Good: A Study in Bootstrapping Low-resource Named Entity Recognizers (#3259)',\n",
       "  'Open Domain Web Keyphrase Extraction Beyond Language Modeling (#1119)',\n",
       "  'TuckER: Tensor Factorization for Knowledge Graph Completion (#990)',\n",
       "  'Weakly Supervised Domain Detection (#TACL-1712)',\n",
       "  'Event Detection with Multi-Order Graph Convolution and Aggregated Attention (#835)',\n",
       "  'Coverage of Information Extraction from Sentences and Paragraphs (#1285)',\n",
       "  'HMEAE: Hierarchical Modular Event Argument Extraction (#2354)',\n",
       "  'Entity, Relation, and Event Extraction with Contextualized Span Representations (#3930)'],\n",
       " 'Semantics': ['Analytical Methods for Interpretable Ultradense Word Embeddings (#75)',\n",
       "  'Investigating Meta-Learning Algorithms for Low-Resource Natural Language Understanding Tasks (#3142)',\n",
       "  'Retrofitting Contextualized Word Embeddings with Paraphrases (#3045)',\n",
       "  'Incorporating Contextual and Syntactic Structures Improves Semantic Similarity Modeling (#3508)'],\n",
       " 'Discourse, Summarization, and Generation': ['Neural Linguistic Steganography (#3399)',\n",
       "  'The Feasibility of Embedding Based Automatic Evaluation for Single Document Summarization (#3018)',\n",
       "  'Attention Optimization for Abstractive Document Summarization (#1918)',\n",
       "  'Rewarding Coreference Resolvers for Being Consistent with World Knowledge (#2020)'],\n",
       " 'Text Mining and NLP Applications': ['An Empirical Study of Incorporating Pseudo Data into Grammatical Error Correction (#740)',\n",
       "  'A Multilingual Topic Model for Learning Weighted Topic Links Across Incomparable Corpora (#1257)',\n",
       "  'Measure Country-Level Socio-Economic Indicators with Streaming News: An Empirical Study (#3730)',\n",
       "  'Towards Extracting Medical Family History from Natural Language Interactions: A New Dataset and Baselines (#2903)',\n",
       "  '(Male, Bachelor) and (Female, Ph.D) have different connotations : Parallelly Annotated Stylistic Language Dataset with Multiple Personas (#3793)',\n",
       "  'Movie Plot Analysis via Turning Point Identification (#244)',\n",
       "  'Latent Suicide Risk Detection on Microblog via Suicide-Oriented Word Embeddings and Layered Attention (#2488)',\n",
       "  'Deep Ordinal Regression for Pledge Specificity Prediction (#1903)',\n",
       "  'Enabling Robust Grammatical Error Correction in New Domains: Datasets, Metrics, and Analyses (#TACL-1677)',\n",
       "  'The Myth of Blind Review Revisited: Experiments on ACL vs. EMNLP (#2233)',\n",
       "  'Uncover Sexual Harassment Patterns from Personal Stories by Joint Key Element Extraction and Categorization (#2653)',\n",
       "  'Identifying Predictive Causal Factors from News Streams (#2864)',\n",
       "  'Training Data Augmentation for Detecting Adverse Drug Reactions in User-Generated Content (#3011)',\n",
       "  'Deep Reinforcement Learning-based Text Anonymization against Private-Attribute Inference (#3160)'],\n",
       " 'Neural Machine Translation': ['Enhancing Context Modeling with a Query-Guided Capsule Network for Document-level NMT (#2416)',\n",
       "  'Simple, Scalable Adaptation for Neural Machine Translation (#3252)',\n",
       "  'Controlling Text Complexity in Neural Machine Translation (#3177)',\n",
       "  'Investigating Multilingual NMT Representations at Scale (#1388)',\n",
       "  'Hierarchical Modeling of Global Context for Document-Level Neural Machine Translation (#1423)'],\n",
       " 'Question Answering': ['Cross-Lingual Machine Reading Comprehension (#8)',\n",
       "  'A Multi-Type Multi-Span Network for Reading Comprehension that Requires Discrete Reasoning (#582)',\n",
       "  'Neural Duplicate Question Detection without Labeled Training Data (#880)',\n",
       "  'Asking Clarification Questions in Knowledge-Based Question Answering (#889)',\n",
       "  'Multi-View Domain Adapted Sentence Embeddings for Low-Resource Unsupervised Duplicate Question Detection (#1646)',\n",
       "  'Interactive Language Learning by Question Answering (#1367)',\n",
       "  'What’s Missing: A Knowledge Gap Guided Approach for Multi-hop Question Answering (#3238)',\n",
       "  'KagNet: Learning to Answer Commonsense Questions with Knowledge-Aware Graph Networks (#436)',\n",
       "  'Learning with Limited Data for Multilingual Reading Comprehension (#3518)',\n",
       "  'A Discrete Hard EM Approach for Weakly Supervised Question Answering (#3778)'],\n",
       " 'Social Media and Computational Social Science': ['Multi-label Categorization of Accounts of Sexism using a Neural Framework (#172)',\n",
       "  'The Trumpiest Trump? Identifying a Subject’s Most Characteristic Tweets (#1462)',\n",
       "  'Finding Microaggressions in the Wild: A Case for Locating Elusive Phenomena in Social Media Posts (#2950)',\n",
       "  'Reinforced Product Metadata Selection for Helpfulness Assessment of Customer Reviews (#694)',\n",
       "  'Learning Invariant Representations of Social Media Users (#3557)'],\n",
       " 'Discourse and Pragmatics': ['A Unified Neural Coherence Model (#1792)',\n",
       "  'Topic-Guided Coherence Modeling for Sentence Ordering by Preserving Global and Local Information (#2642)',\n",
       "  'Neural Generative Rhetorical Structure Parsing (#4060)',\n",
       "  'Weak Supervision for Learning Discourse Structure (#2453)',\n",
       "  'Predicting Discourse Structure using Distant Supervision from Sentiment (#2625)'],\n",
       " 'Tagging, Chunking, Syntax and Parsing': ['Designing and Interpreting Probes with Control Tasks (#4063)',\n",
       "  'Specializing Word Embeddings (for Parsing) by Information Bottleneck (#1357)',\n",
       "  'Deep Contextualized Word Embeddings in Transition-Based and Graph-Based Dependency Parsing - A Tale of Two Parsers Revisited (#2799)',\n",
       "  'Semantic graph parsing with recurrent neural network DAG grammars (#2863)',\n",
       "  '75 Languages, 1 Model: Parsing Universal Dependencies Universally (#1221)'],\n",
       " 'Linguistic Theories, Cognitive Modeling and Psycholinguistics': ['Is the Red Square Big? MALeViC: Modeling Adjectives Leveraging Visual Contexts (#2585)',\n",
       "  'Investigating BERT’s Knowledge of Language: Five Analysis Methods with NPIs (#3650)',\n",
       "  'Representation of Constituents in Neural Language Models: - Coordination Phrase as a Case Study (#3929)',\n",
       "  'Towards Zero-shot Language Modelling (#1745)',\n",
       "  'Neural Network Acceptability Judgments (#TACL-1710)'],\n",
       " 'Machine Translation and Multilinguality': ['Lost in Evaluation: Misleading Benchmarks for Bilingual Dictionary Induction (#1131)',\n",
       "  'Towards Realistic Practices In Low-Resource Natural Language Processing: The Development Set (#1266)',\n",
       "  'Synchronously Generating Two Languages with Interactive Decoding (#1478)',\n",
       "  'On NMT Search Errors and Model Errors: Cat Got Your Tongue? (#1868)',\n",
       "  'Do We Really Need Fully Unsupervised Cross-Lingual Embeddings? (#2459)',\n",
       "  'Weakly-Supervised Concept-based Adversarial Learning for Cross-lingual Word Embeddings (#2491)',\n",
       "  'Aligning Cross-lingual Entities with Multi-Aspect Information (#3541)',\n",
       "  'Contrastive Language Adaptation for Cross-Lingual Stance Detection (#2498)',\n",
       "  'Jointly Learning to Align and Translate with Transformer Models (#422)',\n",
       "  'Understanding Data Augmentation in Neural Machine Translation: Two Perspectives towards Generalization (#2192)',\n",
       "  'Simple and Effective Noisy Channel Modeling for Neural Machine Translation (#2869)',\n",
       "  'MultiFiT: Efficient Multi-lingual Language Model Fine-tuning (#745)',\n",
       "  'Hint-based Training for Non-AutoRegressive Machine Translation (#1064)',\n",
       "  'Two New Evaluation Datasets for Low-Resource Machine Translation: Nepali-English and Sinhala English (#3349)',\n",
       "  'Constant-Time Machine Translation with Conditional Masked Language Models (#1204)',\n",
       "  'Learning to Copy for Automatic Post-Editing (#777)'],\n",
       " 'Reasoning and Question Answering': ['Going on a vacation takes longer than “Going for a walk”: A Study of Temporal Commonsense Understanding (#2533)',\n",
       "  'QAInfomax: Learning Robust Question Answering System by Mutual Information Maximization (#2798)',\n",
       "  'Adapting Meta Knowledge Graph Information for Multi-Hop Reasoning over Few-Shot Relations (#329)',\n",
       "  'How Reasonable are Common-Sense Reasoning Tasks: A Case-Study on the Winograd Schema Challenge and SWAG (#586)'],\n",
       " 'Generation': ['Pun-GAN: Generative Adversarial Network for Pun Generation (#267)',\n",
       "  'Multi-Task Learning with Language Modeling for Question Generation (#3820)',\n",
       "  'Autoregressive Text Generation beyond Feedback Loops (#3506)',\n",
       "  'The Woman Worked as a Babysitter: On Biases in Language Generation (#3874)',\n",
       "  'Counterfactual Story Reasoning and Generation (#3328)',\n",
       "  'Encode, Tag, Realize: High-Precision Text Editing (#2395)',\n",
       "  'Answer-guided and Semantic Coherent Question Generation in Open-domain Conversation (#128)',\n",
       "  'Read, Attend and Comment: A Deep Architecture for Automatic News Comment Generation (#1947)',\n",
       "  'A Topic Augmented Text Generation Model: Joint Learning of Semantics and Structural Features (#2822)',\n",
       "  'A Modular Architecture for Unsupervised Sarcasm Generation (#2725)',\n",
       "  'Interpoetry: Generating Classical Chinese Poems from Vernacular Chinese (#2534)',\n",
       "  'Set to Ordered Text: Generating Discharge Instructions from Medical Billing Codes (#724)'],\n",
       " 'Summarization': ['Summary Cloze: A New Task for Content Selection in Topic-Focused Summarization (#1178)',\n",
       "  'Text Summarization with Pretrained Encoders (#392)',\n",
       "  'How to Write Summaries with Patterns? Learning towards Abstractive Summarization through Prototype Editing (#609)',\n",
       "  'Unsupervised Sentence Summarization using the Information Bottleneck Principle (#3219)',\n",
       "  'Improving Latent Alignment in Text Summarization by Generalizing the Pointer Generator (#3043)'],\n",
       " 'Information Retrieval and Document Analysis': ['Cross-Cultural Transfer Learning for Text Classification (#1036)',\n",
       "  'Combining Unsupervised Pre-training and Annotator Rationales to Improve Low-shot Text Classification (#1190)',\n",
       "  'Projection Sequence Networks for On-Device Text Classification (#3202)',\n",
       "  'Induction Networks for Few-Shot Text Classification (#3562)',\n",
       "  'Benchmarking Zero-shot Text Classification: Datasets, Evaluation and Entailment Approach (#2899)',\n",
       "  'Human-grounded Evaluations of Explanation Methods for Text Classification (#425)',\n",
       "  'A Context-based Framework for Modeling the Role and Function of On-line Resource Citations in Scientific Literature (#793)',\n",
       "  'Adversarial Reprogramming of Text Classification Neural Networks (#28)',\n",
       "  'Document Hashing with Mixture-Prior Generative Models (#1676)',\n",
       "  'Efficient Vector Retrieval under Maximum Inner Product (#3421)'],\n",
       " 'Reasoning': ['Social IQa: Commonsense Reasoning about Social Interactions (#1334)',\n",
       "  'Self-Assembling Modular Networks for Interpretable Multi-Hop Reasoning (#2866)',\n",
       "  'Posing Fair Generalization Tasks for Natural Language Inference (#1413)',\n",
       "  'Everything Happens for a Reason: Discovering the Purpose of Actions in Procedural Text (#3279)',\n",
       "  'CLUTRR: A Diagnostic Benchmark for Inductive Reasoning from Text (#3183)'],\n",
       " 'Syntax, Parsing, and Linguistic Theories': ['Working Hard or Hardly Working: Challenges of Integrating Typology into Neural Dependency Parsers (#3860)',\n",
       "  'Cross-Lingual BERT Transformation for Zero-Shot Dependency Parsing (#1832)',\n",
       "  'Multilingual Grammar Induction with Continuous Language Identification (#3883)',\n",
       "  'Quantifying the Semantic Core of Gender Systems (#2637)'],\n",
       " 'Sentiment and Social Media': ['Perturbation Sensitivity Analysis for Detecting Unintended Model Biases (#3447)',\n",
       "  'Automatically Inferring Gender Associations from Language (#3519)',\n",
       "  'Reporting the Unreported: Event Extraction for Analyzing the Local Representation of Hate Crimes (#3715)',\n",
       "  'Minimally Supervised Learning of Affective Events Using Discourse Relations (#3493)'],\n",
       " 'Phonology, Word Segmentation, and Parsing': ['Constraint-based Learning of Phonological Processes (#451)',\n",
       "  'Detect Camouflaged Spam Content via StoneSkipping: Graph and Text Joint Embedding for Chinese Character Variation Representation (#1340)',\n",
       "  'A Generative Model for Punctuation in Dependency Trees (#TACL-1582)']}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oral_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googlesearch import search\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "from difflib import SequenceMatcher\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def similar(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "\n",
    "def search_arxiv_link(title):\n",
    "    link = None\n",
    "    for j in search(p, tld=\"co.in\", num=10, stop=1, pause=0.5):\n",
    "        if 'arxiv.org/abs' in j:\n",
    "            thepage = urllib.request.urlopen(j)\n",
    "            soup = BeautifulSoup(thepage, \"html.parser\")\n",
    "            if similar(p.lower(), soup.title.text.lower()) > 0.8:\n",
    "                link = j\n",
    "                break\n",
    "            else:\n",
    "                print(\"ERROR\")\n",
    "                print(p[:-15])\n",
    "                print(soup.title.text)\n",
    "    return link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/26 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR\n",
      "Attention is Not Not Exp\n",
      "[1908.04626] Attention is not not Explanation\n",
      "ERROR\n",
      "Adaptively Sparse Trans\n",
      "[1909.00015] Adaptively Sparse Transformers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|▍         | 1/26 [00:35<14:45, 35.42s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR\n",
      "How Contextual are Contextualized Word Represen\n",
      "[1909.00512] How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings\n",
      "ERROR\n",
      "Game Theory Meets Embeddings: a Unified Framework for Word Sense Disambi\n",
      "[1606.07711] A Game-Theoretic Approach to Word Sense Disambiguation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "  8%|▊         | 2/26 [01:03<13:17, 33.24s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR\n",
      "Multi-hop Selector Network for Multi-turn Response Selection in Retrieval-based \n",
      "[1901.01824] Interactive Matching Network for Multi-Turn Response Selection in Retrieval-Based Chatbots\n",
      "ERROR\n",
      "MoEL: Mixture of Empathetic Li\n",
      "[1908.07687] MoEL: Mixture of Empathetic Listeners\n",
      "ERROR\n",
      "TaskMaster Dialog Corpus: Toward a Realistic and Diverse\n",
      "[1909.05358] Taskmaster-1: Toward a Realistic and Diverse Dialog Dataset\n",
      "ERROR\n",
      "MultiDoGO: Multi-Domain Goal-Oriented Di\n",
      "[1905.08743] Transferable Multi-Domain State Generator for Task-Oriented Dialogue Systems\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      " 12%|█▏        | 3/26 [01:30<12:02, 31.39s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR\n",
      "Evaluating adversarial attacks against multiple fact verification\n",
      "[1903.05543] Adversarial attacks against Fact Extraction and VERification\n",
      "ERROR\n",
      "Towards Debiasing Fact Verification\n",
      "[1908.05267] Towards Debiasing Fact Verification Models\n",
      "ERROR\n",
      "Recognizing Conflict Opinions in Aspect-level Sentiment Classification with Dual Attention \n",
      "[1804.06536] Aspect Level Sentiment Classification with Attention-over-Attention Neural Networks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      " 15%|█▌        | 4/26 [02:10<12:28, 34.00s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 19%|█▉        | 5/26 [02:25<09:51, 28.17s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 23%|██▎       | 6/26 [02:50<09:03, 27.19s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR\n",
      "YouMakeup: A Large-Scale Domain-Specific Multimodal Dataset for Fine-Grained Semantic Compr\n",
      "[1806.06193] Large Scale Fine-Grained Categorization and Domain-Specific Transfer Learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      " 27%|██▋       | 7/26 [03:17<08:38, 27.27s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR\n",
      "Fine-Grained Evaluation for Entity\n",
      "[1909.05780] Fine-Grained Entity Typing for Domain Independent Entity Linking\n",
      "ERROR\n",
      "Neural Cross-Lingual Event Detection with Minimal Parallel Re\n",
      "[1808.09861] Neural Cross-Lingual Named Entity Recognition with Minimal Resources\n",
      "ERROR\n",
      "Improving Distantly-Supervised Relation Extraction with Joint Label E\n",
      "[1804.06987] Improving Distantly Supervised Relation Extraction using Word and Entity Based Attention\n",
      "ERROR\n",
      "Leverage Lexical Knowledge for Chinese Named Entity Recognition via Collaborative Graph\n",
      "[1905.01964] Neural Chinese Named Entity Recognition via CNN-LSTM-CRF and Joint Training with Word Segmentation\n",
      "ERROR\n",
      "Weakly Supervised Domain Detecti\n",
      "[1907.11499] Weakly Supervised Domain Detection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      " 31%|███       | 8/26 [04:00<09:36, 32.05s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 35%|███▍      | 9/26 [04:11<07:13, 25.49s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR\n",
      "Neural Linguistic Stegan\n",
      "[1909.01496] Neural Linguistic Steganography\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      " 38%|███▊      | 10/26 [04:19<05:26, 20.43s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR\n",
      "A Multilingual Topic Model for Learning Weighted Topic Links Across Incomparable \n",
      "[1806.04270] Learning Multilingual Topics from Incomparable Corpus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      " 42%|████▏     | 11/26 [04:48<05:42, 22.81s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 46%|████▌     | 12/26 [05:02<04:42, 20.19s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR\n",
      "Hierarchical Modeling of Global Context for Document-Level Neural Machine Tran\n",
      "[1809.01576] Document-Level Neural Machine Translation with Hierarchical Attention Networks\n",
      "ERROR\n",
      "KagNet: Learning to Answer Commonsense Questions with Knowledge-Aware Graph \n",
      "[1909.02151] KagNet: Knowledge-Aware Graph Networks for Commonsense Reasoning\n",
      "ERROR\n",
      "Learning with Limited Data for Multilingual Reading Compre\n",
      "[1809.03275] Multilingual Extractive Reading Comprehension by Runtime Machine Translation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      " 50%|█████     | 13/26 [05:26<04:37, 21.37s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR\n",
      "Multi-label Categorization of Accounts of Sexism using a Neural F\n",
      "[1811.05475] ML-Net: multi-label classification of biomedical texts with deep neural networks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      " 54%|█████▍    | 14/26 [05:37<03:38, 18.20s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR\n",
      "A Unified Neural Coherenc\n",
      "[1909.00349] A Unified Neural Coherence Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      " 58%|█████▊    | 15/26 [05:46<02:52, 15.71s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 62%|██████▏   | 16/26 [06:00<02:32, 15.20s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR\n",
      "Towards Zero-shot Language Mo\n",
      "[1906.08584] Improving Zero-shot Translation with Language-Independent Constraints\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      " 65%|██████▌   | 17/26 [06:14<02:13, 14.84s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR\n",
      "Neural Network Acceptability Judgmen\n",
      "[1805.12471] Neural Network Acceptability Judgments\n",
      "ERROR\n",
      "Understanding Data Augmentation in Neural Machine Translation: Two Perspectives towards General\n",
      "[1905.10523] Soft Contextual Data Augmentation for Neural Machine Translation\n",
      "ERROR\n",
      "Constant-Time Machine Translation with Conditional Masked Language\n",
      "[1904.09324] Mask-Predict: Parallel Decoding of Conditional Masked Language Models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      " 69%|██████▉   | 18/26 [06:54<02:56, 22.12s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 73%|███████▎  | 19/26 [07:04<02:10, 18.65s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR\n",
      "Pun-GAN: Generative Adversarial Network for Pun Ge\n",
      "[1903.03496] A Three-Player GAN: Generating Hard Samples To Improve Classification Networks\n",
      "ERROR\n",
      "Interpoetry: Generating Classical Chinese Poems from Vernacular \n",
      "[1604.01537] Generating Chinese Classical Poems with RNN Encoder-Decoder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      " 77%|███████▋  | 20/26 [07:33<02:10, 21.68s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR\n",
      "Unsupervised Sentence Summarization using the Information Bottleneck Pr\n",
      "[1909.07405] BottleSum: Unsupervised and Self-supervised Sentence Summarization using the Information Bottleneck Principle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      " 81%|████████  | 21/26 [07:44<01:33, 18.64s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 85%|████████▍ | 22/26 [08:09<01:21, 20.40s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR\n",
      "Posing Fair Generalization Tasks for Natural Language In\n",
      "[1909.08940] Improving Generalization by Incorporating Coverage in Natural Language Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      " 88%|████████▊ | 23/26 [08:22<00:54, 18.07s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 92%|█████████▏| 24/26 [08:30<00:30, 15.19s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 96%|█████████▌| 25/26 [08:39<00:13, 13.45s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 26/26 [08:45<00:00, 10.96s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/12 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR\n",
      "Open Relation Extraction: Relational Knowledge Transfer from Supervised Data to Unsupervi\n",
      "[1801.07174] Unsupervised Open Relation Extraction\n",
      "ERROR\n",
      "Self-Attention Enhanced CNNs and Collaborative Curriculum Learning for Distantly Supervised Relation Ext\n",
      "[1804.06987] Improving Distantly Supervised Relation Extraction using Word and Entity Based Attention\n",
      "ERROR\n",
      "Investigating Capsule Network and Semantic Feature on Hyperplanes for Text Classi\n",
      "[1804.00538] Investigating Capsule Networks with Dynamic Routing for Text Classification\n",
      "ERROR\n",
      "Label-Specific Document Representation for Multi-Label Text Classi\n",
      "[1905.10070] Label-aware Document Representation via Hybrid Attention for Extreme Multi-Label Text Classification\n",
      "ERROR\n",
      "Latent-Variable Generative Text Classifiers for Data-Effici\n",
      "[1910.00382] Latent-Variable Generative Models for Data-Efficient Text Classification\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "  8%|▊         | 1/12 [01:14<13:36, 74.26s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR\n",
      "Iterative Dual Domain Adaptation for Neural Machine Tran\n",
      "[1906.00376] Domain Adaptation of Neural Machine Translation by Lexicon Induction\n",
      "ERROR\n",
      "Hierarchical Pointer Net\n",
      "[1908.11571] Hierarchical Pointer Net Parsing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      " 17%|█▋        | 2/12 [02:53<13:36, 81.62s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR\n",
      "Dirichlet Latent Variable Hierarchical Recurrent Encoder-Decoder in Dialogue Ge\n",
      "[1605.06069] A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues\n",
      "ERROR\n",
      "Sampling Matters! An Empirical Study of Negative Sampling Strategies for Learning of Matching Models in Retrieval-based Dialogue \n",
      "[1811.09785] Strategy of the Negative Sampling for Training Retrieval-Based Dialogue Systems\n",
      "ERROR\n",
      "Recurrent Embedding for Neural Machine Tran\n",
      "[1907.08158] Understanding Neural Machine Translation by Simplification: The Case of Encoder-free Models\n",
      "ERROR\n",
      "Low-Resource Neural Machine Translation by Exploiting Multilingualism through Multi-Step Fine-Tuning Using N-way Parallel \n",
      "[1907.03060] Exploiting Out-of-Domain Parallel Data through Multilingual Transfer Learning for Low-Resource Neural Machine Translation\n",
      "ERROR\n",
      "Korean Morphological Analysis with Tied Sequence-to-Sequence Multi-Tas\n",
      "[1805.07946] Morphological analysis using a sequence decoder\n",
      "ERROR\n",
      "Improving Visual Dialog by Learning to Answer Diverse Q\n",
      "[1909.10470] Improving Generative Visual Dialog by Answering Diverse Questions\n",
      "ERROR\n",
      "Cross-lingual Transfer Learning with Data Selection for Large-Scale Spoken Language Under\n",
      "[1904.01825] Cross-lingual transfer learning for spoken language understanding\n",
      "ERROR\n",
      "Efficient Navigation with Language Pre-training and Stochastic S\n",
      "[1909.02244] Robust Navigation with Language Pretraining and Stochastic Sampling\n",
      "ERROR\n",
      "Towards Making a Dependency Pa\n",
      "[1909.01053] Towards Making a Dependency Parser See\n",
      "ERROR\n",
      "Unsupervised Labeled Parsing with Deep Inside-Outside Recursive Autoe\n",
      "[1904.02142] Unsupervised Latent Tree Induction with Deep Inside-Outside Recursive Autoencoders\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      " 25%|██▌       | 3/12 [04:34<13:07, 87.51s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR\n",
      "Span-based Hierarchical Semantic Parsing for Task-Oriented\n",
      "[1810.07942] Semantic Parsing for Task Oriented Dialog using Hierarchical Representations\n",
      "ERROR\n",
      "Adaptive Parameterization for Neural Dialogue Ge\n",
      "[1701.06547] Adversarial Learning for Neural Dialogue Generation\n",
      "ERROR\n",
      "Retrieval-guided Dialogue Response Generation via a Matching-to-Generation Fr\n",
      "[1809.05296] Skeleton-to-Response: Dialogue Generation Guided by Retrieval Memory\n",
      "ERROR\n",
      "Modeling Personalization in Continuous Space for Response Generation via Augmented Wasserstein Autoe\n",
      "[1805.12352] DialogWAE: Multimodal Response Generation with Conditional Wasserstein Auto-Encoder\n",
      "ERROR\n",
      "CoSQL: A Conversational Text-to-SQL Challenge Towards Cross-Domain Natural Language Interfaces to Da\n",
      "[1905.08205] Towards Complex Text-to-SQL in Cross-Domain Database with Intermediate Representation\n",
      "ERROR\n",
      "Low-Rank HOCA: Efficient High-Order Cross-Modal Attention for Video Ca\n",
      "[1804.05448] Watch, Listen, and Describe: Globally and Locally Aligned Cross-Modal Attentions for Video Captioning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      " 33%|███▎      | 4/12 [06:03<11:43, 87.95s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR\n",
      "Ranking and Sampling in Open-domain Question A\n",
      "[1810.00494] Ranking Paragraphs for Improving Answer Recall in Open-Domain Question Answering\n",
      "ERROR\n",
      "Language Models as Knowledge\n",
      "[1909.01066] Language Models as Knowledge Bases?\n",
      "ERROR\n",
      "Quick and (not so) Dirty: Unsupervised Selection of Justification Sentences for Multi-hop Question An\n",
      "[1904.09380] Repurposing Entailment for Multi-Hop Question Answering Tasks\n",
      "ERROR\n",
      "Answering Complex Open-domain Questions Through Iterative Query Gen\n",
      "[1904.09537] PullNet: Open Domain Question Answering with Iterative Retrieval on Knowledge Bases and Text\n",
      "ERROR\n",
      "Original Semantics-Oriented Attention and Deep Fusion Network for Sentence \n",
      "[1805.11360] Semantic Sentence Matching with Densely-connected Recurrent and Co-attentive Information\n",
      "ERROR\n",
      "Query-focused Scenario Const\n",
      "[1909.06877] Query-Focused Scenario Construction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      " 42%|████▏     | 5/12 [07:26<10:05, 86.46s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR\n",
      "NCLS: Neural Cross-Lingual Summa\n",
      "[1909.00156] NCLS: Neural Cross-Lingual Summarization\n",
      "ERROR\n",
      "Improving Question Generation With to the Point \n",
      "[1809.02393] Improving Neural Question Generation using Answer Separation\n",
      "ERROR\n",
      "Domain Adaptive Text Style T\n",
      "[1908.09395] Domain Adaptive Text Style Transfer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      " 50%|█████     | 6/12 [08:42<08:19, 83.30s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR\n",
      "Text Level Graph Neural Network for Text Classi\n",
      "[1809.05679] Graph Convolutional Networks for Text Classification\n",
      "ERROR\n",
      "Neural Topic Model with Reinforcement L\n",
      "[1909.01436] Discriminative Topic Modeling with Logistic LDA\n",
      "ERROR\n",
      "Modelling Stopping Criteria using Poisson Pr\n",
      "[1909.06239] Modelling Stopping Criteria for Search Results using Poisson Processes\n",
      "ERROR\n",
      "Bridging the Defined and the Defining: Exploiting Implicit Lexical Semantic Relations in Definition M\n",
      "[1612.00394] Definition Modeling: Learning to define word embeddings in natural language\n",
      "ERROR\n",
      "Feature-Dependent Confusion Matrices for Low-Resource NER Labeling with Nois\n",
      "[1903.12008] Handling Noisy Labels for Robustly Learning from Self-Training Data for Low-Resource Sequence Labeling\n",
      "ERROR\n",
      "PaLM: A Hybrid Parser and Languag\n",
      "[1909.02134] PaLM: A Hybrid Parser and Language Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      " 58%|█████▊    | 7/12 [10:21<07:20, 88.15s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR\n",
      "Style Transfer for Texts: to Err is Human, but Error Margin\n",
      "[1908.06809] Style Transfer for Texts: Retrain, Report Errors, Compare with Rewrites\n",
      "ERROR\n",
      "Auditing Deep Learning processes through Kernel-based Explanator\n",
      "[1802.01396] To understand deep learning we need to understand kernel learning\n",
      "ERROR\n",
      "Enhancing Recurrent Variational Autoencoders with Mutual Information Estimation for Text Gen\n",
      "[1903.07137] Topic-Guided Variational Autoencoders for Text Generation\n",
      "ERROR\n",
      "Distributionally Robust Language M\n",
      "[1909.02060] Distributionally Robust Language Modeling\n",
      "ERROR\n",
      "Learning Latent Parameters without Human Response Patterns: Item Response Theory with Artificial\n",
      "[1905.10957] Enhancing Item Response Theory for Cognitive Diagnosis\n",
      "ERROR\n",
      "Compositional Generalization for Premitive Substi\n",
      "[1906.05381] Compositional generalization through meta sequence-to-sequence learning\n",
      "ERROR\n",
      "Empirical Study of Transformer's Attention Mechanism via the Lens of\n",
      "[1908.11775] Transformer Dissection: An Unified Understanding for Transformer's Attention via the Lens of Kernel\n",
      "ERROR\n",
      "Revealing the Dark Secrets \n",
      "[1908.08593] Revealing the Dark Secrets of BERT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      " 67%|██████▋   | 8/12 [12:11<06:19, 94.79s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR\n",
      "Countering Language Drift via Gr\n",
      "[1909.04499] Countering Language Drift via Visual Grounding\n",
      "ERROR\n",
      "Unsupervised Domain Adaptation for Political Document A\n",
      "[1904.02817] Unsupervised Domain Adaptation of Contextualized Embeddings for Sequence Labeling\n",
      "ERROR\n",
      "Domain Adaptation for Person-Job Fit with Transferable Deep Global Match\n",
      "[1503.00591] Deep Transfer Network: Unsupervised Domain Adaptation\n",
      "ERROR\n",
      "News2vec: News Network Embedding with Subnode Inf\n",
      "[1904.08157] Compositional Network Embedding\n",
      "ERROR\n",
      "Reviews Meet Graphs: Enhancing User and Item Representations for Recommendation with Hierarchical Attentive Graph Neural \n",
      "[1905.08108] Neural Graph Collaborative Filtering\n",
      "ERROR\n",
      "A Neural Citation Count Prediction Model based on Peer Revi\n",
      "[1811.02129] Modeling and Predicting Citation Count via Recurrent Neural Network with Long Short-Term Memory\n",
      "ERROR\n",
      "Stick to the Facts: Learning towards a Fidelity-oriented E-Commerce Product Description Gen\n",
      "[1903.12457] Towards Knowledge-Based Personalized Product Description Generation in E-commerce\n",
      "ERROR\n",
      "Fine-Grained Entity Typing via Hierarchical Multi Graph Convolutional N\n",
      "[1902.06667] Hierarchical Graph Convolutional Networks for Semi-supervised Node Classification\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      " 75%|███████▌  | 9/12 [13:50<04:47, 95.91s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR\n",
      "MulCode: A Multiplicative Multi-way Model for Compressing Neural Langua\n",
      "[1909.11687] Extreme Language Model Compression with Optimal Subwords and Shared Projections\n",
      "ERROR\n",
      "Specificity-Driven Cascading Approach for Unsupervised Sentiment Modi\n",
      "[1808.07311] Learning Sentiment Memories for Sentiment Modification without Parallel Data\n",
      "ERROR\n",
      "A Dataset of General-Purpose R\n",
      "[1909.00393] A Dataset of General-Purpose Rebuttal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      " 83%|████████▎ | 10/12 [15:22<03:09, 94.77s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR\n",
      "Has Pepperoni and Still Vegan?! Improving Answer Consistency in VQA through Entailed Question Ge\n",
      "[1909.04696] Sunny and Dark Outside?! Improving Answer Consistency in VQA through Entailed Question Generation\n",
      "ERROR\n",
      "Revisiting the Evaluation of Theory of Mind through Question An\n",
      "[1808.09352] Evaluating Theory of Mind in Question Answering\n",
      "ERROR\n",
      "CLAR: Contextualized and Lexicalized Aspect Representation for Non-factoid Question An\n",
      "[1901.03491] Answer Interaction in Non-factoid Question Answering Systems\n",
      "ERROR\n",
      "Question-type Driven Question Gen\n",
      "[1909.00140] Question-type Driven Question Generation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      " 92%|█████████▏| 11/12 [17:13<01:39, 99.56s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR\n",
      "Cross-Sentence N-ary Relation Extraction using Lower-Arity Universal \n",
      "[1708.03743] Cross-Sentence N-ary Relation Extraction with Graph LSTMs\n",
      "ERROR\n",
      "ner and pos when nothing is capi\n",
      "[1903.11222] ner and pos when nothing is capitalized\n",
      "ERROR\n",
      "Adversarial Removal of Demographic Attributes Re\n",
      "[1808.06640] Adversarial Removal of Demographic Attributes from Text Data\n",
      "ERROR\n",
      "Neural News Recommendation with Multi-Head Self-At\n",
      "[1907.05576] Neural News Recommendation with Attentive Multi-View Learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 12/12 [18:48<00:00, 98.37s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "fo = open('accepted_papers.md', 'w')\n",
    "\n",
    "# Oral papers\n",
    "fo.write('## Accepted Papers (Oral)\\n')\n",
    "for t in tqdm(oral_papers.keys()):\n",
    "    fo.write(f'### {t}\\n')\n",
    "    for p in oral_papers[t]:\n",
    "        link = search_arxiv_link(p)\n",
    "        if link:\n",
    "            fo.write(f'- {p} [[arXiv]]({link})\\n')\n",
    "        else:\n",
    "            fo.write(f'- {p}\\n')\n",
    "    fo.write('\\n')\n",
    "\n",
    "# Poster & Demo papers\n",
    "fo.write('## Accepted Papers (Poster & Demo)\\n')\n",
    "for t in tqdm(poster_demo_papers.keys()):\n",
    "    fo.write(f'### {t}\\n')\n",
    "    for p in poster_demo_papers[t]:\n",
    "        link = search_arxiv_link(p)\n",
    "        if link:\n",
    "            fo.write(f'- {p} [arXiv]({link})\\n')\n",
    "        else:\n",
    "            fo.write(f'- {p}\\n')\n",
    "    fo.write('\\n')\n",
    "    \n",
    "fo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
